{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y8RbDerirKXW",
        "outputId": "6c33e740-1f0e-405d-de1e-14a40f9ae646"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import train_test_split  # For initial validation\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import numpy as np\n",
        "\n",
        "# Define the full column names (from the dataset description)\n",
        "full_column_names = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "    'land', 'wrong_fragment', 'urgent', 'hot', 'num_failed_logins',\n",
        "    'logged_in', 'num_compromised', 'root_shell', 'su_attempted',\n",
        "    'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
        "    'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count',\n",
        "    'srv_count', 'serror_rate', 'srv_serror_rate', 'rerror_rate',\n",
        "    'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
        "    'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
        "    'dst_host_serror_rate', 'dst_host_srv_serror_rate',\n",
        "    'dst_host_rerror_rate', 'dst_host_srv_rerror_rate',\n",
        "    'attack_type', 'difficulty_score'\n",
        "]\n",
        "\n",
        "\n",
        "# Define the important features you want to use\n",
        "important_features = [\n",
        "    'duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes',\n",
        "    'logged_in', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate',\n",
        "    'same_srv_rate', 'dst_host_count', 'dst_host_srv_count',\n",
        "    'dst_host_same_srv_rate', 'dst_host_diff_srv_rate',\n",
        "    'dst_host_serror_rate', 'dst_host_srv_serror_rate'\n",
        "]\n",
        "\n",
        "# --- 1. Load Training Data ---\n",
        "train_file_path = 'KDDTrain+.txt'\n",
        "try:\n",
        "    df_train = pd.read_csv(train_file_path, header=None,\n",
        "                           names=full_column_names)\n",
        "    print(f\"Successfully loaded {train_file_path}\")\n",
        "    print(f\"Full training dataset shape: {df_train.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: {train_file_path} not found. Please upload the file.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the training data: {e}\")\n",
        "    exit()\n",
        "\n",
        "# --- 2. Prepare Training Data ---\n",
        "# Extract features and target variable\n",
        "X_train = df_train[important_features].copy()\n",
        "y_train = df_train['attack_type'].apply(\n",
        "    lambda x: 0 if x == 'normal' else 1)  # Binary: 0=normal, 1=attack\n",
        "\n",
        "# Identify categorical and numerical features\n",
        "categorical_features = X_train.select_dtypes(\n",
        "    include=['object', 'category']).columns\n",
        "numerical_features = X_train.select_dtypes(\n",
        "    include=['int64', 'float64']).columns\n",
        "\n",
        "print(\"\\nTarget variable distribution (Train - Binary: 0=normal, 1=attack):\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nShape of selected features (X_train): {X_train.shape}\")\n",
        "print(\n",
        "    f\"\\nSelected Categorical features for OHE: {list(categorical_features)}\")\n",
        "print(f\"Selected Numerical features: {list(numerical_features)}\")\n",
        "\n",
        "# --- 3. Create Preprocessor ---\n",
        "# ColumnTransformer for preprocessing\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', StandardScaler(), numerical_features),\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False),\n",
        "         categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough')  # Or 'drop' if you want to drop other columns\n",
        "\n",
        "# --- 4. Build Model Pipeline ---\n",
        "# Create a pipeline with preprocessing and LightGBM\n",
        "model = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                      ('classifier',\n",
        "                       lgb.LGBMClassifier(random_state=42,\n",
        "                                        n_estimators=100, # Example: Tune!\n",
        "                                        learning_rate=0.1, # Example: Tune!\n",
        "                                        max_depth=10,     # Example: Tune!\n",
        "                                        n_jobs=-1))])  # Use all cores\n",
        "\n",
        "# --- 5. Train the Model ---\n",
        "print(\"\\nTraining the LightGBM model...\")\n",
        "model.fit(X_train, y_train)  # Train the pipeline\n",
        "print(\"Model training complete.\")\n",
        "\n",
        "# --- 6. Load Test Data ---\n",
        "test_file_path = 'KDDTest+.txt'\n",
        "try:\n",
        "    df_test = pd.read_csv(test_file_path, header=None,\n",
        "                           names=full_column_names)\n",
        "    print(f\"\\nSuccessfully loaded {test_file_path}\")\n",
        "    print(f\"Full test dataset shape: {df_test.shape}\")\n",
        "except FileNotFoundError:\n",
        "    print(\n",
        "        f\"Error: {test_file_path} not found. Please upload the file.\")\n",
        "    exit()\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the test data: {e}\")\n",
        "    exit()\n",
        "\n",
        "\n",
        "# --- 7. Prepare Test Data ---\n",
        "# Extract features and target variable from the test set\n",
        "X_test = df_test[important_features].copy()  # Use the SAME features\n",
        "y_test = df_test['attack_type'].apply(\n",
        "    lambda x: 0 if x == 'normal' else 1)  # Binary: 0=normal, 1=attack\n",
        "\n",
        "\n",
        "# --- 8. Preprocess Test Data ---\n",
        "# ***CRITICAL STEP:*** Use the fitted preprocessor from training!\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "\n",
        "# Print the classification report\n",
        "print(\"\\n--- Evaluation on KDDTest+.txt ---\")\n",
        "print(classification_report(y_test, y_pred,\n",
        "                              target_names=['Normal', 'Attack']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tuWraKyvBHfd",
        "outputId": "f895b932-19b5-405b-c86e-0d2aaf05fd16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded KDDTrain+.txt\n",
            "Full training dataset shape: (125973, 43)\n",
            "\n",
            "Target variable distribution (Train - Binary: 0=normal, 1=attack):\n",
            "attack_type\n",
            "0    67343\n",
            "1    58630\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Shape of selected features (X_train): (125973, 18)\n",
            "\n",
            "Selected Categorical features for OHE: ['protocol_type', 'service', 'flag']\n",
            "Selected Numerical features: ['duration', 'src_bytes', 'dst_bytes', 'logged_in', 'count', 'srv_count', 'serror_rate', 'srv_serror_rate', 'same_srv_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate', 'dst_host_diff_srv_rate', 'dst_host_serror_rate', 'dst_host_srv_serror_rate']\n",
            "\n",
            "Training the LightGBM model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 58630, number of negative: 67343\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.022025 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 2459\n",
            "[LightGBM] [Info] Number of data points in the train set: 125973, number of used features: 90\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.465417 -> initscore=-0.138552\n",
            "[LightGBM] [Info] Start training from score -0.138552\n",
            "Model training complete.\n",
            "\n",
            "Successfully loaded KDDTest+.txt\n",
            "Full test dataset shape: (22544, 43)\n",
            "\n",
            "--- Evaluation on KDDTest+.txt ---\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Normal       0.66      0.97      0.79      9711\n",
            "      Attack       0.97      0.63      0.76     12833\n",
            "\n",
            "    accuracy                           0.78     22544\n",
            "   macro avg       0.82      0.80      0.78     22544\n",
            "weighted avg       0.84      0.78      0.77     22544\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10. Batch Prediction from File ---\n",
        "print(\"\\n=== Batch Prediction from File ===\")\n",
        "print(f\"Reading inputs from 'input_data.txt'\")\n",
        "print(\"Note: Lines starting with '#' and blank lines will be ignored\")\n",
        "print(f\"Each valid line should contain {len(important_features)} comma-separated values\")\n",
        "print(\"Expected features:\", \", \".join(important_features))\n",
        "\n",
        "custom_input_path = 'input_data.txt'\n",
        "\n",
        "try:\n",
        "    with open(custom_input_path, 'r') as f:\n",
        "        # Filter out comments and blank lines\n",
        "        lines = [line.strip() for line in f\n",
        "                if line.strip() and not line.strip().startswith('#')]\n",
        "\n",
        "        if not lines:\n",
        "            print(\"\\nâŒ Error: No valid data found in input file\")\n",
        "            print(\"(File is empty or contains only comments/blank lines)\")\n",
        "            exit()\n",
        "\n",
        "        print(f\"\\nFound {len(lines)} valid records to process:\\n\")\n",
        "\n",
        "        results = []\n",
        "        valid_count = 0\n",
        "        for i, line in enumerate(lines, 1):\n",
        "            try:\n",
        "                # Skip empty lines (additional check)\n",
        "                if not line.strip():\n",
        "                    continue\n",
        "\n",
        "                # Skip comment lines (additional check)\n",
        "                if line.strip().startswith('#'):\n",
        "                    continue\n",
        "\n",
        "                values = [x.strip() for x in line.split(',')]\n",
        "\n",
        "                if len(values) != len(important_features):\n",
        "                    print(f\"{i}. âŒ SKIPPED: Expected {len(important_features)} values, got {len(values)}\")\n",
        "                    print(f\"   Problematic line: {line[:80]}{'...' if len(line) > 80 else ''}\")\n",
        "                    continue\n",
        "\n",
        "                # Convert to DataFrame\n",
        "                input_df = pd.DataFrame([values], columns=important_features)\n",
        "\n",
        "                # Type conversion\n",
        "                for col in input_df.columns:\n",
        "                    if col in numerical_features:\n",
        "                        input_df[col] = pd.to_numeric(input_df[col])\n",
        "                    elif col in categorical_features:\n",
        "                        input_df[col] = input_df[col].astype(str)\n",
        "\n",
        "                # Prediction\n",
        "                prediction = model.predict(input_df)[0]\n",
        "                proba = model.predict_proba(input_df)[0]\n",
        "\n",
        "                valid_count += 1\n",
        "                results.append({\n",
        "                    'id': valid_count,  # Use sequential count of valid records\n",
        "                    'prediction': 'Normal' if prediction == 0 else 'Attack',\n",
        "                    'normal_conf': proba[0],\n",
        "                    'attack_conf': proba[1],\n",
        "                    'input': line[:50] + '...' if len(line) > 50 else line\n",
        "                })\n",
        "\n",
        "                print(f\"{valid_count}. âœ“ {results[-1]['prediction']} \"\n",
        "                      f\"(Normal: {proba[0]:.2%}, \"\n",
        "                      f\"Attack: {proba[1]:.2%})\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"{i}. âŒ ERROR: {str(e)}\")\n",
        "                print(f\"   Problematic line: {line[:80]}{'...' if len(line) > 80 else ''}\")\n",
        "                continue\n",
        "\n",
        "        # Display summary\n",
        "        if results:\n",
        "            print(\"\\n\\nðŸ“Š Prediction Summary:\")\n",
        "            print(\"-\" * 65)\n",
        "            print(f\"{'No.':<4} {'Prediction':<10} {'Normal Conf.':<15} {'Attack Conf.':<15} {'Sample Input':<20}\")\n",
        "            print(\"-\" * 65)\n",
        "            for res in results:\n",
        "                print(f\"{res['id']}.  {res['prediction']:<10} {res['normal_conf']:.2%}{'':<6} {res['attack_conf']:.2%}{'':<6} {res['input']}\")\n",
        "\n",
        "            # Statistics\n",
        "            attack_count = sum(1 for res in results if res['prediction'] == 'Attack')\n",
        "            print(\"\\nðŸ” Summary Statistics:\")\n",
        "            print(f\"â€¢ Total valid records processed: {valid_count}/{len(lines)}\")\n",
        "            print(f\"â€¢ Normal traffic detected: {valid_count - attack_count} ({(valid_count - attack_count)/valid_count:.1%})\")\n",
        "            print(f\"â€¢ Attacks detected: {attack_count} ({attack_count/valid_count:.1%})\")\n",
        "        else:\n",
        "            print(\"\\nâš ï¸ No valid predictions were generated\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\nâŒ Error: File '{custom_input_path}' not found\")\n",
        "    print(\"Please create an 'input_data.txt' file with your input data.\")\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Unexpected error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHuR0gGcnkLK",
        "outputId": "cd5d7a9b-a846-4948-a995-2e327cb14c41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Batch Prediction from File ===\n",
            "Reading inputs from 'input_data.txt'\n",
            "Note: Lines starting with '#' and blank lines will be ignored\n",
            "Each valid line should contain 18 comma-separated values\n",
            "Expected features: duration, protocol_type, service, flag, src_bytes, dst_bytes, logged_in, count, srv_count, serror_rate, srv_serror_rate, same_srv_rate, dst_host_count, dst_host_srv_count, dst_host_same_srv_rate, dst_host_diff_srv_rate, dst_host_serror_rate, dst_host_srv_serror_rate\n",
            "\n",
            "Found 12 valid records to process:\n",
            "\n",
            "1. âœ“ Attack (Normal: 0.02%, Attack: 99.98%)\n",
            "2. âœ“ Attack (Normal: 0.02%, Attack: 99.98%)\n",
            "3. âœ“ Normal (Normal: 99.99%, Attack: 0.01%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4. âœ“ Attack (Normal: 0.02%, Attack: 99.98%)\n",
            "5. âœ“ Normal (Normal: 99.85%, Attack: 0.15%)\n",
            "6. âœ“ Attack (Normal: 0.02%, Attack: 99.98%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7. âœ“ Normal (Normal: 99.94%, Attack: 0.06%)\n",
            "8. âœ“ Normal (Normal: 70.34%, Attack: 29.66%)\n",
            "9. âœ“ Normal (Normal: 100.00%, Attack: 0.00%)\n",
            "10. âœ“ Normal (Normal: 90.17%, Attack: 9.83%)\n",
            "11. âœ“ Normal (Normal: 100.00%, Attack: 0.00%)\n",
            "12. âœ“ Normal (Normal: 71.88%, Attack: 28.12%)\n",
            "\n",
            "\n",
            "ðŸ“Š Prediction Summary:\n",
            "-----------------------------------------------------------------\n",
            "No.  Prediction Normal Conf.    Attack Conf.    Sample Input        \n",
            "-----------------------------------------------------------------\n",
            "1.  Attack     0.02%       99.98%       0,tcp,http,SF,0,0,0,123,6,1.00,1.00,0.05,255,26,0....\n",
            "2.  Attack     0.02%       99.98%       0,tcp,http,S0,0,0,0,270,23,1.00,1.00,0.09,255,23,0...\n",
            "3.  Normal     99.99%       0.01%       0,tcp,http,SF,232,8153,1,5,5,0.20,0.20,1.00,30,255...\n",
            "4.  Attack     0.02%       99.98%       0,tcp,http,SF,0,0,0,123,6,1.00,1.00,0.05,255,26,0....\n",
            "5.  Normal     99.85%       0.15%       0,tcp,ftp_data,SF,491,0,0,2,2,0.00,0.00,1.00,150,2...\n",
            "6.  Attack     0.02%       99.98%       0,tcp,http,S0,0,0,0,270,23,1.00,1.00,0.09,255,23,0...\n",
            "7.  Normal     99.94%       0.06%       0,udp,domain,SF,146,0,0,13,1,0.00,0.00,0.08,255,1,...\n",
            "8.  Normal     70.34%       29.66%       0,icmp,echo,SF,1032,0,0,1,1,0.00,0.00,1.00,255,1,1...\n",
            "9.  Normal     100.00%       0.00%       0,tcp,smtp,SF,199,420,1,30,32,0.00,0.00,1.00,255,2...\n",
            "10.  Normal     90.17%       9.83%       0,tcp,ftp,REJ,112,0,0,3,1,0.00,0.00,0.33,3,1,0.33,...\n",
            "11.  Normal     100.00%       0.00%       0,tcp,ssh,SF,343,1178,1,9,10,0.00,0.00,1.00,157,25...\n",
            "12.  Normal     71.88%       28.12%       0,tcp,telnet,SF,1003,0,1,1,1,0.00,0.00,1.00,1,1,1....\n",
            "\n",
            "ðŸ” Summary Statistics:\n",
            "â€¢ Total valid records processed: 12/12\n",
            "â€¢ Normal traffic detected: 8 (66.7%)\n",
            "â€¢ Attacks detected: 4 (33.3%)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 10. Interactive Custom Input Prediction ---\n",
        "print(\"\\n=== Custom Input Prediction ===\")\n",
        "print(\"Enter values for the following features (comma-separated):\")\n",
        "print(\", \".join(important_features))\n",
        "\n",
        "# Get input from user\n",
        "user_input = input(\"\\nEnter values: \").strip()\n",
        "\n",
        "try:\n",
        "    # Process the input\n",
        "    values = [x.strip() for x in user_input.split(',')]\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    input_df = pd.DataFrame([values], columns=important_features)\n",
        "\n",
        "    # Convert types to match training data\n",
        "    for col in input_df.columns:\n",
        "        if col in numerical_features:\n",
        "            input_df[col] = pd.to_numeric(input_df[col])\n",
        "        elif col in categorical_features:\n",
        "            input_df[col] = input_df[col].astype(str)\n",
        "\n",
        "    # Predict using the pipeline\n",
        "    prediction = model.predict(input_df)[0]\n",
        "    pred_label = 'Normal' if prediction == 0 else 'Attack'\n",
        "    proba = model.predict_proba(input_df)[0]\n",
        "\n",
        "    print(f\"\\nðŸ” Prediction: {pred_label}\")\n",
        "    print(f\"   Confidence: Normal: {proba[0]:.2%}, Attack: {proba[1]:.2%}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\nâŒ Error processing input: {e}\")\n",
        "    print(\"Please make sure you entered all 18 values in the correct order.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLTjuu7oed3e",
        "outputId": "601e5753-6764-4c34-9a56-49d6f3146102"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Custom Input Prediction ===\n",
            "Enter values for the following features (comma-separated):\n",
            "duration, protocol_type, service, flag, src_bytes, dst_bytes, logged_in, count, srv_count, serror_rate, srv_serror_rate, same_srv_rate, dst_host_count, dst_host_srv_count, dst_host_same_srv_rate, dst_host_diff_srv_rate, dst_host_serror_rate, dst_host_srv_serror_rate\n",
            "\n",
            "Enter values: 0,tcp,http,SF,0,0,0,123,6,1.00,1.00,0.05,255,26,0.10,0.05,1.00,1.00\n",
            "\n",
            "ðŸ” Prediction: Attack\n",
            "   Confidence: Normal: 0.02%, Attack: 99.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    }
  ]
}